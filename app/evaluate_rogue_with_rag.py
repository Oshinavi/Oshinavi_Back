import json
import os
import time
from typing import List, Dict
from statistics import mean

# â– â–  ä»¥ä¸‹ã€ã‚ã‚‰ã‹ã˜ã‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»æ§‹æˆæ¸ˆã¿ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’æƒ³å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#   pip install sentence-transformers faiss-cpu fugashi rank_bm25 langchain-anthropic langchain
#   ã¾ãŸã¯ ChatOpenAI ç”¨ã« langchain-chat_models, openai ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ ãªã©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from fugashi import Tagger
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
import faiss

from langchain_anthropic import ChatAnthropic
from langchain.chat_models import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain.chains import LLMChain

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  â¤ BASE_DIR: ì´ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í„°ë¦¬(= app/) ê²½ë¡œ
BASE_DIR = os.path.dirname(__file__)

# â¤ .env íŒŒì¼ ìœ„ì¹˜ (app/config/settings.env)
ENV_PATH = os.path.join(BASE_DIR, "config", "settings.env")

# â¤ settings.env ì— ì í˜€ ìˆëŠ” KEY=VALUE í˜•ì‹ì˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ì½ì–´ì„œ os.environì— ì„¤ì •
if os.path.exists(ENV_PATH):
    with open(ENV_PATH, "r", encoding="utf-8") as f:
        for raw_line in f:
            line = raw_line.strip()
            # ë¹ˆ ì¤„ í˜¹ì€ ì£¼ì„(#)ì´ë©´ ê±´ë„ˆë›°ê¸°
            if not line or line.startswith("#"):
                continue
            if "=" in line:
                key, val = line.split("=", 1)
                os.environ[key.strip()] = val.strip()
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class PromptType(str):
    TRANSLATE = "translate"
    # ï¼ˆå¿…è¦ã«å¿œã˜ã¦ä»–ã‚‚ã‚ã‚‹ãŒä»Šå›ã¯ç¿»è¨³ã®ã¿ä½¿ã†ï¼‰


SYSTEM_PROMPTS: Dict[PromptType, str] = {
    PromptType.TRANSLATE: """
You are an AI translator. Your task is to translate Twitter's Japanese text into natural, cute Korean, while preserving specific elements.

Follow these instructions precisely:
1.  Preserve the following elements EXACTLY as they appear in the original text (do NOT translate or alter them):
    * Hashtags (tokens starting with #)
    * Mentions (tokens starting with @)
    * The literal prefix "RT @user:" (including the space)
    * Emojis (emojis like ğŸ˜‚, âœ¨, â¤ï¸, etc.)

2.  Translate ALL other Japanese characters and words into natural, cute Korean.
    * Pay close attention to adverbs and nuanced expressions (e.g., ã€ŒæœãŸã—ã¦ã€, ã€Œã‚„ã£ã±ã‚Šã€, ã€Œã¾ã•ã‹ã€) and translate them naturally, reflecting their nuance and emotional tone within the context.
    * Do not skip seemingly minor words or fillers; translate everything necessary for natural flow.
    * Use contextual paraphrasing to ensure the translated Korean is smooth, natural, and captures the original meaning accurately, rather than just a literal word-for-word translation.

3.  Output ONLY the final translated Korean text. Do not include the original text, explanations, or any other information.
""".strip(),
}

def get_few_shot_examples(pt: PromptType) -> str:
    """
    å®Ÿé‹ç”¨ã§ã¯ few_shot.json ã‹ã‚‰èª­ã¿è¾¼ã‚€ãŒã€ã“ã“ã§ã¯çœç•¥ã—ã¦ç©ºæ–‡å­—ã‚’è¿”ã™ã€‚
    """
    return ""

def _build_system_prompt(prompt_type: PromptType, **kwargs) -> str:
    few = get_few_shot_examples(prompt_type)
    base = SYSTEM_PROMPTS[prompt_type].strip()
    if kwargs:
        base = base.format(**kwargs)
    return f"{few}\n\n{base}"

class DummyRAGService:
    """
    RAGãªã—ï¼ˆç¿»è¨³ãƒã‚§ãƒ¼ãƒ³ã«æ¸¡ã™ã¨ get_context ãŒå¸¸ã«ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™ï¼‰
    """
    def get_context(self, query: str) -> List[str]:
        return []

class RAGService:
    """
    FAISS + BM25 ã‚’çµ„ã¿åˆã‚ã›ãŸ RAGService å®Ÿè£…
    ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚ã‚‰ã‹ã˜ã‚ä½œã£ã¦ãŠãã“ã¨ï¼‰
    """
    def __init__(
        self,
        index_path: str,
        meta_path: str,
        top_k: int = 15,
        lexical_weight: float = 0.7,
    ):
        # 1) FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰
        self.index = faiss.read_index(index_path)

        # 2) ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿(JSON)ã‚’ãƒ­ãƒ¼ãƒ‰
        with open(meta_path, encoding="utf-8") as f:
            self.metadata: List[Dict[str, str]] = json.load(f)
        self.source_texts = [entry["text"] for entry in self.metadata]

        # 3) Embedding ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
        self.embedder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

        # 4) å½¢æ…‹ç´ è§£æå™¨ã‚’åˆæœŸåŒ–
        self.tagger = Tagger()

        # 5) BM25 ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰
        tokenized_corpus = [self._tokenize(text) for text in self.source_texts]
        self.bm25 = BM25Okapi(tokenized_corpus)

        self.top_k = top_k
        self.lexical_weight = lexical_weight

    def _tokenize(self, text: str) -> List[str]:
        """
        æ—¥æœ¬èªã‚’å½¢æ…‹ç´ å˜ä½ã§ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        """
        return [word.surface for word in self.tagger(text)]

    def get_context(self, query: str) -> List[str]:
        """
        ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦ FAISS + BM25 ã‚’çµ„ã¿åˆã‚ã›ã¦ä¸Šä½ top_k ã® "åŸæ–‡ â†’ ç¿»è¨³" ã‚’è¿”ã™
        """
        # 1) Semantic æ¤œç´¢ (L2æ­£è¦åŒ–æ¸ˆã¿ so Cosine)
        query_embedding = self.embedder.encode([query])
        sims, indices = self.index.search(query_embedding, self.top_k)
        sims = sims[0]
        semantic_indices = [int(i) for i in indices[0] if 0 <= i < len(self.source_texts)]

        # 2) Lexical æ¤œç´¢ (BM25)
        tokenized_query = self._tokenize(query)
        bm25_scores = self.bm25.get_scores(tokenized_query)
        top_lex_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[: self.top_k]

        # 3) å€™è£œ indices ã® union
        candidate_indices = set(semantic_indices) | set(top_lex_indices)

        # 4) Semantic similarity æ­£è¦åŒ–
        max_sim = float(sims.max()) if sims.size > 0 else 1.0
        sem_score_map = {semantic_indices[i]: sims[i] / max_sim for i in range(len(semantic_indices))}

        # 5) BM25 æ­£è¦åŒ–
        max_bm = float(bm25_scores.max()) if bm25_scores.size > 0 else 1.0

        # 6) Combined ã‚¹ã‚³ã‚¢è¨ˆç®—
        combined_scores: List[(int, float)] = []
        for idx in candidate_indices:
            sem_sc = sem_score_map.get(idx, 0.0)
            lex_sc = (bm25_scores[idx] / max_bm) if max_bm > 0 else 0.0
            total_sc = sem_sc + self.lexical_weight * lex_sc
            combined_scores.append((idx, total_sc))

        # 7) ä¸Šä½ top_k ã‚’é¸æŠ
        combined_scores.sort(key=lambda x: x[1], reverse=True)
        selected = [idx for idx, _ in combined_scores[: self.top_k]]

        # 8) â€œåŸæ–‡ â†’ ç¿»è¨³â€ ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¿”å´
        return [f"{self.metadata[i]['text']} â†’ {self.metadata[i]['translation']}" for i in selected]

class TranslationChain:
    """
    â€œBaselineâ€ ã¨ â€œRAGâ€ ä¸¡æ–¹ã§ä½¿ã†å…±é€šã®ç¿»è¨³ãƒã‚§ãƒ¼ãƒ³ã€‚
    RAGãªã—ã®å ´åˆã¯ DummyRAGService ã‚’æ¸¡ã™ã ã‘ã§ã€contexts ãŒç©ºã«ãªã‚Šã€
    å®Ÿè³ªçš„ã«ã¯â€œãŸã ã® Claude ç¿»è¨³â€ã¨åŒç­‰ã®æŒ™å‹•ã«ãªã‚‹ã€‚
    """
    def __init__(self, rag_service, model_name: str = "claude-3-7-sonnet-20250219"):
        self.rag = rag_service

        # â–¶ ChatAnthropic ì‚¬ìš© ì‹œ, ì´ë¯¸ os.environ["ANTHROPIC_API_KEY"] ë¥¼ ì½ì–´ì™”ìœ¼ë¯€ë¡œ
        #    ë³„ë„ë¡œ ì „ë‹¬í•˜ì§€ ì•Šì•„ë„ ë‚´ë¶€ì—ì„œ í•´ë‹¹ í™˜ê²½ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤.
        self.llm = ChatAnthropic(
            model_name=model_name,
            timeout=120,
            temperature=0.3,
            stop=["\n\nHuman:"],
        )

        prompt = PromptTemplate(
            input_variables=["system", "contexts", "text"],
            template="""
<|system|>
{system}

### Reference dictionary:
{contexts}

<|user|>
{text}
""".strip(),
        )
        self.chain = LLMChain(llm=self.llm, prompt=prompt, output_key="translation")

    def run(self, text: str, timestamp: str) -> str:
        # â€œç¿»è¨³â€å°‚ç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        system = _build_system_prompt(PromptType.TRANSLATE)
        contexts = self._build_contexts(text)
        return self.chain.predict(system=system, contexts=contexts, text=text)

    def _build_contexts(self, text: str) -> str:
        """
        RAGService ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ â€œ- ã€œâ€ ã®ãƒªã‚¹ãƒˆå½¢å¼ã«ã—ã¦è¿”ã™
        """
        contexts = self.rag.get_context(text)
        if not contexts:
            return ""
        return "\n".join(f"- {c}" for c in contexts)

# â– â–  ROUGE ã‚¹ã‚³ã‚¢è¨ˆç®—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def tokenize(text: str) -> List[str]:
    """
    éæ—¥æœ¬èªå°‚ç”¨ã®éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã€‚
    éŸ“å›½èªãƒ»è‹±èªã¯ç©ºç™½åŒºåˆ‡ã‚Šã§ååˆ†ã€‚æ—¥æœ¬èªã¯ã“ã“ã§ã¯ä½¿ã‚ãªã„ã®ã§ç„¡è¦–ã€‚
    """
    return text.strip().split()

def get_ngrams(tokens: List[str], n: int) -> Dict[tuple, int]:
    counts = {}
    for i in range(len(tokens) - n + 1):
        gram = tuple(tokens[i : i + n])
        counts[gram] = counts.get(gram, 0) + 1
    return counts

def rouge_n(reference: str, candidate: str, n: int) -> Dict[str, float]:
    ref_tokens  = tokenize(reference)
    cand_tokens = tokenize(candidate)

    ref_ngrams  = get_ngrams(ref_tokens, n)
    cand_ngrams = get_ngrams(cand_tokens, n)

    overlap = 0
    for gram, cnt in cand_ngrams.items():
        overlap += min(cnt, ref_ngrams.get(gram, 0))

    total_ref  = sum(ref_ngrams.values())
    total_cand = sum(cand_ngrams.values())

    recall    = overlap / total_ref  if total_ref  > 0 else 0.0
    precision = overlap / total_cand if total_cand > 0 else 0.0
    f1        = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0

    return {"precision": precision, "recall": recall, "f1": f1}

def lcs_length(ref_tokens: List[str], cand_tokens: List[str]) -> int:
    m = len(ref_tokens)
    n = len(cand_tokens)
    dp = [[0] * (n + 1) for _ in range(m + 1)]
    for i in range(m - 1, -1, -1):
        for j in range(n - 1, -1, -1):
            if ref_tokens[i] == cand_tokens[j]:
                dp[i][j] = dp[i + 1][j + 1] + 1
            else:
                dp[i][j] = max(dp[i + 1][j], dp[i][j + 1])
    return dp[0][0]

def rouge_l(reference: str, candidate: str) -> Dict[str, float]:
    ref_tokens  = tokenize(reference)
    cand_tokens = tokenize(candidate)
    lcs = lcs_length(ref_tokens, cand_tokens)

    recall    = lcs / len(ref_tokens)  if len(ref_tokens)  > 0 else 0.0
    precision = lcs / len(cand_tokens) if len(cand_tokens) > 0 else 0.0
    f1        = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0

    return {"precision": precision, "recall": recall, "f1": f1}

# â– â–  ãƒ¡ã‚¤ãƒ³å‡¦ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_dataset(jsonl_path: str) -> List[Dict[str, str]]:
    """
    JSONL ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€
    å„è¡Œã« {'source': ..., 'reference': ...} ãŒã‚ã‚‹æƒ³å®šã€‚
    """
    data = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            obj = json.loads(line)
            # å¿…è¦æœ€ä½é™ã®ã‚­ãƒ¼ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if 'source' not in obj or 'reference' not in obj:
                raise ValueError("ê° í–‰ì— 'source' ì™€ 'reference' í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            data.append(obj)
    return data

def compute_rouge_scores(
    items: List[Dict[str, str]],
    baseline_chain: TranslationChain,
    rag_chain: TranslationChain,
    timestamp: str = None
) -> Dict[str, Dict[str, float]]:
    """
    items: List[{'source': JapaneseOriginal, 'reference': CorrectKorean}]
    baseline_chain: DummyRAG ã‚’ä½¿ã£ãŸç¿»è¨³ãƒã‚§ãƒ¼ãƒ³
    rag_chain: RAGService ã‚’ä½¿ã£ãŸç¿»è¨³ãƒã‚§ãƒ¼ãƒ³
    timestamp: ScheduleChain ç”¨ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã ãŒã€ç¿»è¨³ã«ã¯ä¸è¦ãªã®ã§ None ã‚’æ¸¡ã›ã° OK
    """
    scores = {
        'baseline': {'rouge1': [], 'rouge2': [], 'rougeL': []},
        'rag':      {'rouge1': [], 'rouge2': [], 'rougeL': []}
    }

    for idx, item in enumerate(items, 1):
        src = item['source']
        ref = item['reference']

        # 1) Baseline ç¿»è¨³ï¼ˆRAGãªã—ï¼‰
        try:
            baseline_out = baseline_chain.run(src, timestamp or "")
        except Exception as e:
            print(f"[Warning] Baseline ç¿»è¨³ã§ã‚¨ãƒ©ãƒ¼ at {idx}: {e}")
            baseline_out = ""

        # 2) RAGã‚ã‚Š ç¿»è¨³
        try:
            rag_out = rag_chain.run(src, timestamp or "")
        except Exception as e:
            print(f"[Warning] RAG ç¿»è¨³ã§ã‚¨ãƒ©ãƒ¼ at {idx}: {e}")
            rag_out = ""

        # 3) ROUGE-1,2,L è¨ˆç®—
        r1_base = rouge_n(ref, baseline_out, 1)['f1']
        r1_rag  = rouge_n(ref, rag_out, 1)['f1']

        r2_base = rouge_n(ref, baseline_out, 2)['f1']
        r2_rag  = rouge_n(ref, rag_out, 2)['f1']

        rl_base = rouge_l(ref, baseline_out)['f1']
        rl_rag  = rouge_l(ref, rag_out)['f1']

        scores['baseline']['rouge1'].append(r1_base)
        scores['baseline']['rouge2'].append(r2_base)
        scores['baseline']['rougeL'].append(rl_base)

        scores['rag']['rouge1'].append(r1_rag)
        scores['rag']['rouge2'].append(r2_rag)
        scores['rag']['rougeL'].append(rl_rag)

        # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°ï¼ˆå¿…è¦ã«å¿œã˜ã¦ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆå¯ï¼‰
        print(f"[{idx}/{len(items)}] Source: {src}")
        print(f"  Baseline â†’ {baseline_out}")
        print(f"  RAG      â†’ {rag_out}")
        print(f"  REF      â†’ {ref}")
        print(f"  â–¶ ROUGE1_base: {r1_base:.4f}, ROUGE1_rag: {r1_rag:.4f}")
        print(f"  â–¶ ROUGE2_base: {r2_base:.4f}, ROUGE2_rag: {r2_rag:.4f}")
        print(f"  â–¶ ROUGEL_base: {rl_base:.4f}, ROUGEL_rag: {rl_rag:.4f}")
        print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    # å…¨æ–‡ä¾‹ã‚’é›†è¨ˆã—ã¦å¹³å‡å€¤ã‚’è¨ˆç®—
    avg = {
        'baseline': {key: mean(vals) if vals else 0.0 for key, vals in scores['baseline'].items()},
        'rag':      {key: mean(vals) if vals else 0.0 for key, vals in scores['rag'].items()}
    }
    return avg

if __name__ == "__main__":
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¨­å®šéƒ¨åˆ† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1) JSONL ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    JSONL_PATH = os.path.join(BASE_DIR, "translations.jsonl")

    # 2) RAG ç”¨ã«äº‹å‰ã«æ§‹ç¯‰ã—ãŸ FAISS index ã¨ metadata JSON ã®ãƒ‘ã‚¹
    INDEX_PATH = os.path.join(BASE_DIR, "rag_data", "vector_store", "faiss_index.bin")
    META_PATH  = os.path.join(BASE_DIR, "rag_data", "vector_store", "metadata.json")

    # 3) ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«åï¼ˆå¿…è¦ã«å¿œã˜ã¦æ›¸ãæ›ãˆï¼‰
    CLAUDE_MODEL = "claude-3-7-sonnet-20250219"
    # ã‚‚ã— OpenAI ã® ChatGPT ã§å›ã—ãŸã„å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«:
    # CLAUDE_MODEL = None
    #  ãã®å ´åˆã€TranslationChain ã® llm ã‚’ ChatOpenAI(... ) ã«æ›¸ãæ›ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RAGService ã¨ å„ç¿»è¨³ãƒã‚§ãƒ¼ãƒ³ ã®åˆæœŸåŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ã€ŒBaselineã€(DummyRAG) ç”¨ãƒã‚§ãƒ¼ãƒ³
    dummy_rag = DummyRAGService()
    baseline_chain = TranslationChain(rag_service=dummy_rag, model_name=CLAUDE_MODEL)

    # ã€ŒRAGã‚ã‚Šã€ç”¨ãƒã‚§ãƒ¼ãƒ³
    rag_service = RAGService(
        index_path=INDEX_PATH,
        meta_path=META_PATH,
        top_k=15,
        lexical_weight=0.7
    )
    rag_chain = TranslationChain(rag_service=rag_service, model_name=CLAUDE_MODEL)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    dataset = load_dataset(JSONL_PATH)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë²ˆì—­ ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    baseline_outputs: List[Dict[str, str]] = []
    rag_outputs: List[Dict[str, str]] = []

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DUMMY RAG ê³¼ RAG ì ìš© ë²ˆì—­ ë° ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for idx, item in enumerate(dataset, 1):
        src = item['source']
        # Baseline ë²ˆì—­
        try:
            baseline_out = baseline_chain.run(src, "")
        except Exception as e:
            print(f"[Warning] Baseline ë²ˆì—­ ì—ëŸ¬ at {idx}: {e}")
            baseline_out = ""
        # RAG ì ìš© ë²ˆì—­
        try:
            rag_out = rag_chain.run(src, "")
        except Exception as e:
            print(f"[Warning] RAG ë²ˆì—­ ì—ëŸ¬ at {idx}: {e}")
            rag_out = ""

        # ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
        baseline_outputs.append({"source": src, "translation": baseline_out})
        rag_outputs.append({"source": src, "translation": rag_out})

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë²ˆì—­ ê²°ê³¼ë¥¼ JSONL íŒŒì¼ë¡œ ì“°ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    baseline_jsonl_path = os.path.join(BASE_DIR, "baseline_translations.jsonl")
    rag_jsonl_path      = os.path.join(BASE_DIR, "rag_translations.jsonl")

    with open(baseline_jsonl_path, "w", encoding="utf-8") as fw_baseline:
        for rec in baseline_outputs:
            fw_baseline.write(json.dumps(rec, ensure_ascii=False) + "\n")

    with open(rag_jsonl_path, "w", encoding="utf-8") as fw_rag:
        for rec in rag_outputs:
            fw_rag.write(json.dumps(rec, ensure_ascii=False) + "\n")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ROUGE í‰ê°€ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    avg_scores = compute_rouge_scores(dataset, baseline_chain, rag_chain, timestamp="")

    print("\n==== í‰ê·  ROUGE F1 ìŠ¤ì½”ì–´ (ìµœì¢… ê²°ê³¼) ====")
    print("Baseline (RAGãªã—) í‰ê· ")
    for k, v in avg_scores['baseline'].items():
        print(f"  {k.upper():>7}: {v:.4f}")

    print("\nRAGã‚ã‚Š í‰ê· ")
    for k, v in avg_scores['rag'].items():
        print(f"  {k.upper():>7}: {v:.4f}")

    print(f"\nâ–¶ Baseline ë²ˆì—­ ê²°ê³¼: {baseline_jsonl_path}")
    print(f"â–¶ RAG ë²ˆì—­ ê²°ê³¼:      {rag_jsonl_path}")